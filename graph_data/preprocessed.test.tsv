	recognition of proper nouns ; japanese text processing ; morphological analysis problem ; proper nouns ; japanese text	<task> <task> <task> <otherscientificterm> <material>	2 1 1 ; 3 0 4 ; 0 0 2	recognition of <otherscientificterm_3> in <material_4> has been studied as a part of the more general problem of <task_2> in <task_1> -lrb- -lsb- 1 -rsb- -lsb- 2 -rsb- -rrb- . it has also been studied in the framework of japanese information extraction -lrb- -lsb- 3 -rsb- -rrb- in recent years . our approach to the multi-lingual evaluation task -lrb- met -rrb- for <material_4> is to consider the given task as a <task_2> in japanese . our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e. named entity -lrb- ne -rrb- items in the <material_4> . the analyzer is called `` amorph '' . amorph recognizes ne items in two stages : dictionary lookup and rule application . first , it uses several kinds of dictionaries to segment and tag japanese character strings . second , based on the information resulting from the dictionary lookup stage , a set of rules is applied to the segmented strings in order to identify ne items . when a segment is found to be an ne item , this information is added to the segment and it is used to generate the final output .
	3 -- d stereo reconstruction scheme ; 3 -- d shape ; priori geometric constraints ; image information	<method> <otherscientificterm> <otherscientificterm> <otherscientificterm>	2 0 0 ; 3 1 1	we propose to incorporate a <otherscientificterm_2> in a <method_0> to cope with the many cases where <otherscientificterm_3> alone is not sufficient to accurately recover <otherscientificterm_1> . our approach is based on the iterative deformation of a 3 -- d surface mesh to minimize an objective function . we show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices . structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme . they improve the reconstruction results and enforce their consistency with a priori knowledge about object shape . the strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3 -- d reconstruction .
	lack of structures in traditional n-gram models ; lack of structures ; n-gram models ; 	<task> <otherscientificterm> <method> <task>	3 1 3 ; 3 2 3	this work proposes a new research direction to address the <task_0> . it is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus . labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge . bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus . this posterior encodes sparse se-lectional preferences between a head word and its dependents . the model is evaluated on english and czech newspaper texts , and is then validated on french broadcast news transcriptions .
	human interaction with data sources ; listen-communicate-show ;  ; .	<task> <task> <method> <method>	1 1 0 ; 2 0 3 ; 2 1 2	listen-communicate-show -lrb- lcs -rrb- is a new paradigm for <task_0> . we integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources <method_3> we have built and will demonstrate an application of this approach called lcs-marine <method_3> using lcs-marine , tactical personnel can converse with their logistics system to place a supply or information request <method_3> the request is passed to a mobile , intelligent agent for execution at the appropriate database <method_3> requestors can also instruct the system to notify them when the status of a request changes or when a request is complete <method_3> we have demonstrated this capability in several field exercises with the marines and are currently developing applications of this technology in new domains .
	automated interpretation of nominal compounds ; domain independent model ; nominal compounds ; constituents . ;  ; english	<task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>	4 1 4 ; 4 3 4 ; 4 1 3 ; 5 3 2 ; 1 1 0 ; 4 4 4	a <method_1> is proposed for the <task_0> in <material_5> . this model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal <otherscientificterm_3> in particular , we make extensive use of pustejovsky 's principles concerning the predicative information associated with nominals . we argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information . we explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts , provided that complementary semantic information are retrieved .
	thus can find image regions ; detecting interest points ; detectors incorporate histogram-based ; histogram information ; distribution in	<metric> <task> <method> <otherscientificterm> <method>	0 2 2 ; 3 1 1	we present a new method for <task_1> using <otherscientificterm_3> . unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our <method_2> representations , and <metric_0> that present a distinct <method_4> the neighborhood . the proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and blur . the experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness . an extension of our method to space-time interest point detection for action classification is also presented .
	plume 's approach ; restricted domain parser ; instantiation . ; 	<method> <method> <method> <task>	2 1 3 ; 3 3 3 ; 3 1 3 ; 3 1 2 ; 0 5 1 ; 3 6 3	we have implemented a <method_1> called <method_0> . building on previous work at carnegie-mellon university e.g. -lsb- 4 , 5 , 8 -rsb- , <method_0> to parsing is based on semantic caseframe <method_2> this has the advantages of efficiency on grammatical input , and robustness in the face of ungrammatical input . while <method_0> is well adapted to simple declarative and imperative utterances , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage . this paper outlines <method_0> as it currently exists and describes our detailed design for extending <method_0> to handle passives , relative clauses , and interrogatives in a general manner .
	labelled bracket f-score ; unlexicalized parser ; negra corpus ; suffix analysis ; german ;  ; smoothing	<metric> <method> <material> <method> <material> <metric> <method>	1 1 4 ; 0 2 1 ; 5 2 5 ; 3 1 1 ; 6 6 3 ; 6 1 1 ; 2 2 1 ; 5 1 5	in this paper , we present an <method_1> for <material_4> which employs <method_6> and <method_3> to achieve a <metric_0> of 76.2 , higher than previously reported results on the <material_2> . in addition to the high accuracy of the model , the use of <method_6> in an <method_1> allows us to better examine the interplay between <method_6> and parsing results .
	relations between named entities ; lexical and syntactic features ; unsupervised learning approach ;  ; eigenvectors	<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>	3 1 3 ; 3 3 4 ; 3 2 3 ; 2 1 0 ; 3 4 3 ; 1 1 2	this paper presents an <method_2> to disambiguate various <otherscientificterm_0> by use of various <otherscientificterm_1> from the contexts . it works by calculating <otherscientificterm_4> of an adjacency graph 's laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the <otherscientificterm_4> . experiment results on ace corpora show that this spectral clustering based approach outperforms the other clustering methods .
	mathematical formalism ; trees ;  ; strings ; graphs ; dags	<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	2 5 2 ; 3 6 1 ; 2 1 2 ; 5 6 4 ; 1 6 5 ; 2 6 2	this paper proposes a generic <method_0> for the combination of various structures : <otherscientificterm_3> , <otherscientificterm_1> , <otherscientificterm_5> , <otherscientificterm_4> , and products of them . the polarization of the objects of the elementary structures controls the saturation of the final structure . this formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , tag , hpsg and lfg .
	high-resolution parallel inner-product computation ; mixed-signal paradigm ; image processing ;  ; kernels	<task> <method> <task> <method> <method>	1 1 4 ; 3 1 3 ; 4 1 2 ; 3 2 3 ; 1 1 0 ; 3 0 3	a <method_1> is presented for <task_0> in very high dimensions , suitable for efficient implementation of <method_4> in <task_2> . at the core of the externally digital architecture is a high-density , low-power analog array performing binary-binary partial matrix-vector multiplication . full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products . a random modulation scheme produces near-bernoulli statistics even for highly correlated inputs . the approach is validated with real image data , and with experimental results from a cid/dram analog array prototype in 0.5 cents m cmos .
	projective unifocal , bifo-cal , and trifocal tensors ; affine case ; registered tensors ; 	<method> <otherscientificterm> <otherscientificterm> <otherscientificterm>	3 1 3 ; 0 1 1	in this paper we specialize the <method_0> to the <otherscientificterm_1> , and show how the <method_0> obtained relate to the <otherscientificterm_2> encountered in previous work . this enables us to obtain an affine specialization of known projective relations connecting points and lines across two or three views . in the simpler case of affine cameras we give neccessary and sufficient constraints on the components of the trifocal tensor , together with a simple geometric interpretation . finally , we show how the estimation of the <method_0> from point correspondences is achieved through factorization , and discuss the estimation from line correspondences .
	for a ; whole ; 	<material> <method> <task>	2 6 2 ; 2 1 2	this paper presents an algorithm for selecting an appropriate classifier word <material_0> noun . in thai language , it frequently happens that there is fluctuation in the choice of classifier <material_0> given concrete noun , both from the point of view of the <method_1> speech community and individual speakers . basically , there is no exact rule for classifier selection . as far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun . registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation . we propose a corpus-based method -lrb- biber ,1993 ; nagao ,1993 ; smadja ,1993 -rrb- which generates noun classifier associations -lrb- nca -rrb- to overcome the problems in classifier assignment and semantic construction of noun phrase . the nca is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences .
	perception of transparent objects ;  ; images	<task> <otherscientificterm> <material>	1 3 1 ; 1 4 1 ; 1 1 1 ; 2 1 0 ; 1 2 1 ; 1 0 1	the <task_0> from <material_2> is known to be a very hard problem in vision . given a single image , it is difficult to even detect the presence of transparent objects in the scene . in this paper , we explore what can be said about transparent objects by a moving observer . we show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene . we present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion . the objects can be complex in that they may be composed of multiple layers with different refractive indices . we have conducted numerous simulations to verify the practical feasibility of our algorithm . we have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy .
	visual models of 3d object categories ; generative proba-bilistic framework ; appearance information ; geometric constraints ; 	<task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>	2 1 1 ; 4 1 4 ; 3 1 1 ; 1 1 0 ; 2 6 3 ; 4 6 4 ; 4 2 4 ; 4 4 4	we propose a novel <method_1> for learning <task_0> by combining <otherscientificterm_2> and <otherscientificterm_3> . objects are represented as a coherent ensemble of parts that are consistent under 3d viewpoint transformations . each part is a collection of salient image features . a generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints . contrary to most of the existing mixture of viewpoints models , our model establishes explicit correspondences of parts across different viewpoints of the object class . given a new image , detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects . our approach is among the first to propose a <method_1> for 3d object categorization . we test our algorithm on the detection task and the viewpoint classification task by using '' car '' category from both the savarese et al. 2007 and pascal voc 2006 datasets . we show promising results in both the detection and viewpoint classification tasks on these two challenging datasets .
	ambiguity packing and stochastic disambiguation techniques ; lexical-functional grammars ; sentence condensation ; selection . ; 	<method> <method> <task> <method> <method>	4 1 4 ; 0 1 2 ; 0 1 1 ; 3 1 4 ; 3 6 4 ; 4 6 4 ; 4 2 4 ; 4 4 4	we present an application of <method_0> for <method_1> to the domain of <task_2> . our system incorporates a linguistic parser/generator for lfg , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output <method_3> furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of <task_2> systems . an experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings . overall summarization quality of the proposed system is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .
	robust principal component analysis -lrb- robust pca -rrb- problem ; low rank part ; machine learning applications ; data matrix ; sparse residual ; 	<method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>	1 6 4 ; 5 0 5 ; 0 1 2 ; 1 0 3 ; 4 0 3 ; 5 2 5 ; 5 6 5 ; 5 3 5 ; 5 1 5	the <method_0> has been considered in many <task_2> , where the goal is to decompose the <otherscientificterm_3> to a <otherscientificterm_1> plus a <otherscientificterm_4> . while current approaches are developed by only considering the low rank plus sparse structure , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such information help robust pca . thus , in this paper , we study the problem of robust pca with side information , where both prior structure and features of entities are exploited for recovery . we propose a convex problem to incorporate side information in robust pca and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions . in particular , our guarantee suggests that a substantial amount of low rank matrices , which can not be recovered by standard robust pca , become re-coverable by our proposed method . the result theoretically justifies the effectiveness of features in robust pca . in addition , we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information .
	discourse processing algorithms ; demonstrative expressions ;  ; english	<method> <otherscientificterm> <otherscientificterm> <material>	1 3 3 ; 2 1 2	this paper presents necessary and sufficient conditions for the use of <otherscientificterm_1> in <material_3> and discusses implications for current <method_0> . we examine a broad range of texts to show how the distribution of demonstrative forms and functions is genre dependent . this research is part of a larger study of anaphoric expressions , the results of which will be incorporated into a natural language generation system .
	expressive speech communication ; 	<task> <material>	1 1 1	in the study of <task_0> , it is commonly accepted that the emotion perceived by the listener is a good approximation of the intended emotion conveyed by the speaker . this paper analyzes the validity of this assumption by comparing the mismatches between the assessments made by na ¨ ıve listeners and by the speakers that generated the data . the analysis is based on the hypothesis that people are better decoders of their own emotions . therefore , self-assessments will be closer to the intended emotions . using the iemocap database , discrete -lrb- categorical -rrb- and continuous -lrb- attribute -rrb- emotional assessments evaluated by the actors and na ¨ ıve listeners are compared . the results indicate that there is a mismatch between the expression and perception of emotion . the speakers in the database assigned their own emotions to more specific emotional categories , which led to more extreme values in the activation-valence space .
	convolution kernel over parse trees ; syntactic structure information ; captured by the ; relation extraction ; . ; 	<method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task>	2 3 4 ; 2 1 5 ; 5 2 5 ; 1 1 3 ; 0 1 1 ; 5 4 5 ; 5 1 5	this paper proposes to use a <method_0> to model <otherscientificterm_1> for <task_3> . our study reveals that the syntactic structure features embedded in a parse tree are very effective for <task_3> and these features can be well <otherscientificterm_2> convolution tree kernel <otherscientificterm_4> evaluation on the ace 2003 corpus shows that the <method_0> can achieve comparable performance with the previous best-reported feature-based methods on the 24 ace relation subtypes <otherscientificterm_4> it also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ace relation major types .
	automatically inducing a combinatory categorial grammar lexicon ; combinatory categorial grammar lexicon ; turkish dependency treebank ; 	<task> <otherscientificterm> <material> <material>	3 4 3 ; 3 5 3 ; 1 0 2 ; 3 0 3	this paper presents the results of <task_0> from a <material_2> . the fact that turkish is an agglutinating free word order language presents a challenge for language theories . we explored possible ways to obtain a compact lexicon , consistent with ccg principles , from a treebank which is an order of magnitude smaller than penn wsj .
	email communication ; sentence extraction ;  ; summarization	<material> <method> <task> <task>	1 1 3 ; 2 3 2 ; 2 1 2	while <method_1> as an approach to <task_3> has been shown to work in documents of certain genres , because of the conversational nature of <material_0> where utterances are made in relation to one made previously , <method_1> may not capture the necessary segments of dialogue that would make a summary coherent . in this paper , we present our work on the detection of question-answer pairs in an email conversation for the task of email <task_3> . we show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing .
	object detection ; 	<task> <otherscientificterm>	1 4 1 ; 1 1 1 ; 1 2 1	in this paper we discuss <task_0> when only a small number of training examples are given . specifically , we show how to incorporate a simple prior on the distribution of natural images into support vector machines . svms are known to be robust to overfitting ; however , a few training examples usually do not represent well the structure of the class . thus the resulting detectors are not robust and highly depend on the choice of the training examples . we incorporate the prior on natural images by requiring that the separating hyperplane will not only yield a wide margin , but also that the corresponding positive half space will have a low probability to contain natural images -lrb- the background -rrb- . our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel svm when trained on 10 positive and 10 negative examples .
	unified framework ;  ; reasoning ; clustering	<method> <method> <task> <task>	1 5 1 ; 0 1 2 ; 1 6 1	although the study of <task_3> is centered around an intuitively compelling goal , it has been very difficult to develop a <method_0> for <task_2> about it at a technical level , and profoundly diverse approaches to <task_3> abound in the research community . here we suggest a formal perspective on the difficulty in finding such a unification , in the form of an impossibility theorem : for a set of three simple properties , we show that there is no <task_3> function satisfying all three . relaxations of these properties expose some of the interesting -lrb- and unavoidable -rrb- trade-offs at work in well-studied <task_3> techniques such as single-linkage , sum-of-pairs , k-means , and k-median .
	independent and relevant event-based extractive mutli-document summarization approaches ; 	<method> <method>	1 1 1	we investigate <method_0> . in this paper , events are defined as event terms and associated event elements . with independent approach , we identify important contents by frequency of events . with relevant approach , we identify important contents by pagerank algorithm on the event map constructed from documents . experimental results are encouraging .
	dense sub-pixel camera-projector correspondence ; relative geometry ; photometric calibration ; scanning method ; 	<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric>	4 1 4 ; 4 3 4 ; 4 5 4 ; 3 1 0 ; 4 6 4 ; 4 2 4	we present a <method_3> that recovers <otherscientificterm_0> without requiring any <otherscientificterm_2> nor preliminary knowledge of their <otherscientificterm_1> . subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns . we use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities . simulated and experimental results show that our method recovers scene geometry with high subpixel precision , and that it can handle many challenges of active reconstruction systems . we compare our results to state of the art methods such as mi-cro phase shifting and modulated phase shifting .
	acquiring adjectival subcategorization frames ; subcategorization frames ; english corpus data ; statistical parser ; 	<task> <otherscientificterm> <material> <method> <otherscientificterm>	4 6 4 ; 4 2 4 ; 4 1 4 ; 3 1 4	this paper describes a novel system for <task_0> and associated frequency information from <material_2> . the system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations -lrb- grs -rrb- in the output of a robust <method_3> . it uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica . the experiments show that the system is able to detect scf types with 70 % precision and 66 % recall rate . a new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
	multilingual speech and language applications ; machine transliteration/back-transliteration ; between two ; 	<task> <task> <task> <method>	1 1 0 ; 2 1 3 ; 3 1 3 ; 3 2 3	machine transliteration/back-transliteration plays an important role in many <task_0> . in this paper , a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping -lrb- dom -rrb- <task_2> different languages is presented . under this framework , a joint source-channel transliteration model , also called n-gram transliteration model -lrb- n-gram tm -rrb- , is further proposed to model the transliteration process . we evaluate the proposed methods through several transliteration/backtransliteration experiments for english/chinese and english/japanese language pairs . our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly .
	analog programmable array processor ; complex programmable spatio-temporal dynamics ; bio-inspired model ; vertebrate retina ; vlsi	<task> <otherscientificterm> <method> <otherscientificterm> <task>	2 1 1 ; 3 1 2 ; 1 3 4 ; 2 1 0	a <method_2> for an <task_0> , based on studies on the <otherscientificterm_3> , has permitted the realization of <otherscientificterm_1> in <task_4> . this model mimics the way in which images are processed in the visual pathway , rendering a feasible alternative for the implementation of early vision applications in standard technologies . a prototype chip has been designed and fabricated in a 0.5 µm standard cmos process . computing power per area and power consumption is amongst the highest reported for a single chip . design challenges , trade-offs and some experimental results are presented in this paper .
	 ; determiners	<method> <method>	0 3 0 ; 0 1 0	determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the global meaning of a sentence , even if not in a precise way . another problem with determiners is their inherent ambiguity . in this paper we propose a logical formalism , which , among other things , is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear .
	verbal and nonverbal means ; embodied conversational agents ; human-computer interaction ; common ground ;  ; grounding	<method> <method> <task> <task> <otherscientificterm> <task>	4 0 4 ; 4 1 4 ; 0 1 5 ; 3 1 2 ; 4 6 4	we investigate the <method_0> for <task_5> , and propose a design for <method_1> that relies on both kinds of signals to establish <task_3> in <task_2> . we analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task . the distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded , and the overall pattern reflected a monitoring of lack of negative feedback . based on these results , we present an eca that uses verbal and nonverbal <task_5> acts to update dialogue state .
	speech recognition output ; sentence boundary detection ;  ; speech	<otherscientificterm> <task> <method> <material>	2 4 2 ; 2 5 2 ; 2 3 2 ; 3 1 1 ; 2 1 2 ; 1 1 0 ; 2 6 2 ; 2 2 2	sentence boundary detection in <material_3> is important for enriching <otherscientificterm_0> , making <otherscientificterm_0> easier for humans to read and downstream modules to process . in previous work , we have developed hidden markov model -lrb- hmm -rrb- and maximum entropy -lrb- maxent -rrb- classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries . in this paper , we evaluate the use of a conditional random field -lrb- crf -rrb- for this task and relate results with this model to our prior work . we evaluate across two corpora -lrb- conversational telephone <material_3> and broadcast news <material_3> -rrb- on both human transcriptions and <otherscientificterm_0> . in general , our crf model yields a lower error rate than the hmm and max-ent models on the nist sentence boundary detection task in <material_3> , although <otherscientificterm_0> is interesting to note that the best results are achieved by three-way voting among the classifiers . this probably occurs because each model has different strengths and weaknesses for modeling the knowledge sources .
	camera handoff in wide-area surveillance scenarios ; settings with ptz cameras . ; ptz cameras ; 	<task> <otherscientificterm> <otherscientificterm> <otherscientificterm>	3 4 3 ; 3 1 3	we propose a novel approach to associate objects across multiple <otherscientificterm_2> that can be used to perform <task_0> . while previous approaches relied on geometric , appearance , or correlation-based information for establishing correspondences between static cameras , they each have well-known limitations and are not extendable to wide-area <otherscientificterm_1> in our approach , the slave camera only passively follows the target -lrb- by loose registration with the master -rrb- and bootstraps itself from its own incoming imagery , thus effectively circumventing the problems faced by previous approaches and avoiding the need to perform any model transfer . towards this goal , we also propose a novel multiple instance learning -lrb- mil -rrb- formulation for the problem based on the logistic softmax function of covariance-based region features within a map estimation framework . we demonstrate our approach with multiple ptz camera sequences in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches .
	specialized regression problem ; sampling probabilities ;  ; databases ; records	<task> <otherscientificterm> <material> <material> <material>	1 1 4 ; 4 0 3 ; 0 1 1 ; 2 1 2 ; 2 6 2 ; 2 2 2	this paper solves a <task_0> to obtain <otherscientificterm_1> for <material_4> in <material_3> . the goal is to sample a small set of <material_4> over which evaluating aggregate queries can be done both efficiently and accurately . we provide a principled and provable solution for this problem ; it is parameterless and requires no data insights . unlike standard regression problems , the loss is inversely proportional to the regressed-to values . moreover , a cost zero solution always exists and can only be excluded by hard budget constraints . a unique form of reg-ularization is also needed . we provide an efficient and simple regularized empirical risk minimization -lrb- erm -rrb- algorithm along with a theoretical generalization result . our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards .
	probabilistic finite automaton ; probabilistic context-free grammar ; kullback-leibler distance ; 	<otherscientificterm> <method> <method> <method>	3 3 3 ; 3 1 3 ; 3 0 3 ; 1 4 0	we consider the problem of computing the <method_2> , also called the <method_2> , between a <method_1> and a <otherscientificterm_0> . we show that there is a closed-form -lrb- analytical -rrb- solution for one part of the <method_2> , viz the cross-entropy . we discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata .
	pose invariance ; face recognition ; achieve robustness ; wide variability ; face motion ;  ; illumination	<task> <task> <method> <material> <task> <otherscientificterm> <task>	5 0 5 ; 6 6 0 ; 5 2 5 ; 5 6 5 ; 6 0 1 ; 5 3 5 ; 5 4 5 ; 2 1 4 ; 2 6 5 ; 5 1 5 ; 0 0 1	in spite of over two decades of intense research , <task_6> and <task_0> remain prohibitively challenging aspects of <task_1> for most practical applications . the objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , pose and user motion pattern have a <material_3> and face images are of low resolution . in particular there are three areas of novelty : -lrb- i -rrb- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme <task_6> changes ; -lrb- ii -rrb- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -lrb- iii -rrb- we introduce an accurate video sequence '' reillumination '' algorithm to <method_2> to <task_4> patterns in video . we describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme <task_6> , pose and head motion variation . on this challenging data set our system consistently demonstrated a nearly perfect recognition rate -lrb- over 99.7 % on all three databases -rrb- , significantly out-performing state-of-the-art commercial software and methods from the literature .
	minimum bayes-risk decoding ; statistical machine translation ; that measure ; 	<method> <task> <method> <otherscientificterm>	3 1 3 ; 0 1 1 ; 3 0 3 ; 3 2 3	we present <method_0> for <task_1> . this statistical approach aims to minimize expected loss of translation errors under loss functions <method_2> translation performance . we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an mt system , and syntactic structure from parse-trees of source and target language sentences . we report the performance of the mbr decoders on a chinese-to-english translation task . our results show that mbr decoding can be used to tune statistical mt performance for specific loss functions .
	addressee identification in four-participants face-to-face meetings ; naive bayes classifiers ; bayesian network ; 	<task> <method> <method> <otherscientificterm>	3 1 3 ; 2 1 0 ; 1 1 0 ; 1 6 2 ; 3 6 3	we present results on <task_0> using <method_2> and <method_1> . first , we investigate how well the addressee of a dialogue act can be predicted based on gaze , utterance and conversational context features . then , we explore whether information about meeting context can aid classifiers ' performances . both classifiers perform the best when conversational context and utterance features are combined with speaker 's gaze information . the classifiers show little gain from information about meeting context .
	compositional classes of paraphrases ; automatic candidate generation ; manual judgement ; class-oriented framework ; paraphrase examples ; 	<task> <method> <method> <method> <material> <material>	1 1 4 ; 3 1 4 ; 1 6 2 ; 2 1 4 ; 3 1 0	towards deep analysis of <task_0> , we have examined a <method_3> for collecting <material_4> , in which <material_4> are collected for each paraphrase class separately by means of <method_1> and <method_2> . our preliminary experiments on building a paraphrase corpus have so far been producing promising results , which we have evaluated according to cost-efficiency , exhaustiveness , and reliability .
	machine translation systems ; evaluation of human language learners ; automated evaluation techniques	<method> <task> <method>	2 1 1	the purpose of this research is to test the efficacy of applying <method_2> , originally devised for the <task_1> , to the output of <method_0> . we believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems . this , the first experiment in a series of experiments , looks at the intelligibility of mt output . a language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words . even more illuminating was the factors on which the assessors made their decisions . we tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output . subjects were given a set of up to six extracts of translated newswire text . some of the extracts were expert human translations , others were machine translation outputs . the subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation . additionally , they were asked to mark the word at which they made this decision . the results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .
	bare slice disambiguation ; machine learning approach ;  ; a ; dialogue	<task> <method> <method> <otherscientificterm> <material>	2 4 2 ; 2 5 2 ; 4 1 0 ; 2 3 2 ; 2 1 2 ; 2 0 2 ; 1 1 0	this paper presents a <method_1> to <task_0> in <material_4> . we extract <otherscientificterm_3> set of heuristic principles from <otherscientificterm_3> corpus-based sample and formulate them as probabilistic horn clauses . we then use the predicates of such clauses to create <otherscientificterm_3> set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , <otherscientificterm_3> rule-based learning algorithm , and timbl , <otherscientificterm_3> memory-based system . both learners perform well , yielding similar success rates of approx 90 % . the results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our horn clauses can be learnt automatically from these features .
	word similarity measures ; -rrb- at a ; evaluated directly ; evaluation criterion ; 	<metric> <task> <metric> <metric> <metric>	2 1 1 ; 4 1 4 ; 4 6 4 ; 3 1 0 ; 4 2 2 ; 4 2 4	we suggest a new goal and <metric_3> for <metric_0> . the new criterion -- meaning-entailing substitutability -- fits the needs of semantic-oriented nlp applications and can be <metric_2> -lrb- independent of an application <task_1> good level of human agreement . motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality . finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance .
	reflections in image sequences ; layer at each ; image sequences ;  ; reflections	<otherscientificterm> <method> <material> <task> <otherscientificterm>	3 3 3 ; 3 1 3 ; 3 6 3	reflections in <material_2> consist of several layers superimposed over each other . this phenomenon causes many image processing techniques to fail as they assume the presence of only one <method_1> examined site e.g. motion estimation and object recognition . this work presents an automated technique for detecting reflections in <material_2> by analyzing motion trajectories of feature points . it models reflection as regions containing two different layers moving over each other . we present a strong detector based on combining a set of weak detectors . we use novel priors , generate sparse and dense detection maps and our results show high detection rate with rejection to pathological motion and occlusion .
	reconstructing the motion of a 3d articulated tree ; 2d point correspondences ; frames . ; temporal prior ; 	<task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	4 1 4 ; 4 3 4 ; 4 1 2 ; 1 1 0 ; 4 2 4 ; 4 4 4	this paper considers the problem of <task_0> from <otherscientificterm_1> subject to some <otherscientificterm_3> . hitherto , smooth motion has been encouraged using a trajectory basis , yielding a hard combinatorial problem with time complexity growing exponentially in the number of <otherscientificterm_2> branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality . however , they provide no guarantee of being more efficient than exhaustive search . inspired by recent work which reconstructs general trajectories using compact high-pass filters , we develop a dynamic programming approach which scales linearly in the number of frames , leveraging the intrinsically local nature of filter interactions . extension to affine projection enables reconstruction without estimating cameras .
	topical blog post retrieval ; ranking blog posts ; blog posts ;  ; relevance	<task> <task> <material> <task> <metric>	3 1 3 ; 3 2 3 ; 0 5 1 ; 4 3 2 ; 3 0 3	topical blog post retrieval is the task of <task_1> with respect to their <metric_4> for a given topic . to improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process . we consider two groups of indicators : post level -lrb- determined using information about individual <material_2> only -rrb- and blog level -lrb- determined using information from the underlying blogs -rrb- . we describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models . experiments on the trec blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness ; the best performance is achieved when combining them .
	game records of expert players ; board game of go ; 	<material> <task> <otherscientificterm>	2 6 2 ; 2 0 2 ; 0 1 1 ; 2 1 2	we investigate the problem of learning to predict moves in the <task_1> from <material_0> . in particular , we obtain a probability distribution over legal moves for professional play in a given position . this distribution has numerous applications in computer go , including serving as an efficient stand-alone go player . it would also be effective as a move selector and move sorter for game tree search and as a training tool for go players . our method has two major components : a -rrb- a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b -rrb- a bayesian learning algorithm -lrb- in two variants -rrb- that learns a distribution over the values of a move given a board position based on the local pattern context . the system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional go players in 34 % of test positions .
	automatically acquiring english topic signatures ; tend to ; it . ;  ; of	<task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	3 1 3 ; 3 2 3 ; 3 5 3 ; 3 0 3 ; 3 6 3	we present a novel approach for <task_0> . given a particular concept , or word sense , a topic signature is a set <otherscientificterm_4> words that <otherscientificterm_1> co-occur with <otherscientificterm_2> topic signatures can be useful in a number <otherscientificterm_4> natural language processing -lrb- nlp -rrb- applications , such as word sense disambiguation -lrb- wsd -rrb- and text summarisation . our method takes advantage <otherscientificterm_4> the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount <otherscientificterm_4> chinese text available in corpora and on the web . we evaluated the topic signatures on a wsd task , where we trained a second-order vector cooccurrence algorithm on standard wsd datasets , with promising results .
	of the matrices in ; joint matrix triangularization ; signal processing ; joint eigenstructure ; machine learning ; 	<task> <task> <task> <otherscientificterm> <task> <otherscientificterm>	2 6 4 ; 5 6 5 ; 1 1 3 ; 3 1 4 ; 3 1 2 ; 5 1 5	joint matrix triangularization is often used for estimating the <otherscientificterm_3> of a set m of matrices , with applications in <task_2> and <task_4> . we consider the problem of approximate joint matrix triangularization when the matrices in m are jointly diagonalizable and real , but we only observe a set m ' of noise perturbed versions <task_0> m . our main result is a first-order upper bound on the distance between any approximate joint triangularizer <task_0> m ' and any exact joint triangularizer <task_0> m . the bound depends only on the observable matrices in m ' and the noise level . in particular , it does not depend on optimization specific properties of the triangularizer , such as its proximity to critical points , that are typical of existing bounds in the literature . to our knowledge , this is the first a posteriori bound for joint matrix decomposition . we demonstrate the bound on synthetic data for which the ground truth is known .
	psycholinguistic literature ; syntactic priming	<otherscientificterm> <otherscientificterm>	0 1 1	the <otherscientificterm_0> provides evidence for <otherscientificterm_1> , i.e. , the tendency to repeat structures . this paper describes a method for incorporating priming into an incremental probabilistic parser . three models are compared , which involve priming of rules between sentences , within sentences , and within coordinate structures . these models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .
	learned confidence measures ; outlier removal ; quality improvement ; stereo vision	<method> <task> <task> <task>	2 0 3 ; 0 1 2 ; 1 0 3 ; 0 1 1 ; 1 6 2	learned confidence measures gain increasing importance for <task_1> and <task_2> in <task_3> . however , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , active sensing devices and/or synthetic scenes . to overcome this problem , we propose a new , flexible , and scalable way for generating training data that only requires a set of stereo images as input . the key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm . this enables us to generate a huge amount of training data in a fully automated manner . among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the kitti2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .
	learning in autonomous agents ; domain-speciic models of actions ; planning systems	<task> <method> <task>	2 1 1 ; 0 1 1	an important area of <task_0> is the ability to learn <method_1> to be used by <task_2> . in this paper , we present methods by which an agent learns action models from its own experience and from its observation of a domain expert . these methods diier from previous work in the area in two ways : the use of an action model formalism which is better suited to the needs of a re-active agent , and successful implementation of noise-handling mechanisms . training instances are generated from experience and observation , and a variant of golem is used to learn action models from these instances . the integrated learning system has been experimentally validated in simulated construction and ooce domains .
	integrating automatic q/a applications into real-world environments ; interactive question-answering system ; information related ; system ; how ; ferret	<task> <method> <method> <method> <method> <method>	5 1 0 ; 5 5 1	this paper describes <method_5> , an <method_1> designed to address the challenges of <task_0> . <method_5> utilizes a novel approach to q/a known as predictive questioning which attempts to identify the questions -lrb- and answers -rrb- that users need by analyzing <method_4> a user interacts with a <method_3> while gathering <method_2> to a particular scenario .
	automatic abstracting systems ; training resources ; 	<task> <material> <method>	1 1 0 ; 2 1 2	in order to build robust <task_0> , there is a need for better <material_1> than are currently available . in this paper , we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way . the seven categories of the scheme are based on rhetorical moves of argumentation . our experimental results show that the scheme is stable , reproducible and intuitive to use .
	low-level feature analysis ; shape information ; automated segmentation ; . ;  ; images	<method> <otherscientificterm> <task> <method> <task> <material>	4 1 4 ; 4 1 3 ; 3 1 4 ; 5 1 2 ; 4 6 4	the <task_2> of <material_5> into semantically meaningful parts requires <otherscientificterm_1> since <method_0> alone often fails to reach this goal <method_3> we introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge <method_3> the combined approach is formulated in the framework of bayesian statistics to account for the robust-ness requirement in image understanding <method_3> experimental evidence shows that semantically meaningful segments are inferred , even when image data alone gives rise to ambiguous segmentations .
	natural language environment ; human-machine interactions ; 	<otherscientificterm> <task> <otherscientificterm>	2 5 2 ; 0 3 1 ; 2 1 2	the goal of this work is the enrichment of <task_1> in a <otherscientificterm_0> . because a speaker and listener can not be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker 's utterance . these mistakes can lead to various kinds of misunderstandings between speaker and listener , including reference failures or failure to understand the speaker 's intention . we call these misunderstandings miscommunication . such mistakes can slow , and possibly break down , communication . our goal is to recognize and isolate such miscommunications and circumvent them . this paper highlights a particular class of miscommunication -- reference problems -- by describing a case study and techniques for avoiding failures of reference . we want to illustrate a framework less restrictive than earlier ones by allowing a speaker leeway in forming an utterance about a task and in determining the conversational vehicle to deliver it . the paper also promotes a new view for extensional reference .
	combination methods ; 	<method> <method>	1 1 1 ; 1 2 1	combination methods are an effective way of improving system performance . this paper examines the benefits of system combination for unsupervised wsd . we investigate several voting - and arbiter-based combination strategies over a diverse pool of unsupervised wsd systems . our combination methods rely on predominant senses which are derived automatically from raw text . experiments using the semcor and senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .
	be used to learn ; supervised training data ; information extraction techniques ; a primarily ; fashion . ; 	<task> <material> <method> <material> <material> <otherscientificterm>	5 1 5 ; 4 5 0 ; 5 2 5 ; 3 6 4 ; 5 4 5 ; 5 1 0 ; 3 5 0 ; 1 1 2	the applicability of many current <method_2> is severely limited by the need for <material_1> . we demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can <task_0> effective models in <material_3> unsupervised <material_4> although hidden markov models -lrb- hmms -rrb- provide a suitable generative model for field structured text , general unsupervised hmm learning fails to learn useful structure in either of our domains . however , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions . in both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .
	natural language question answering system ;  ; chat-80	<method> <method> <method>	1 1 1 ; 1 5 1 ; 2 5 0	this paper gives an overall account of a prototype <method_0> , called <method_2> . <method_2> has been designed to be both efficient and easily adaptable to a variety of applications . the system is implemented entirely in prolog , a programming language based on logic . with the aid of a logic-based grammar formalism called extraposition grammars , <method_2> translates english questions into the prolog subset of logic . the resulting logical expression is then transformed by a planning algorithm into efficient prolog , cf. query optimisation in a relational database . finally , the prolog form is executed to yield the answer .
	well-segmented 3d skeleton data ; localizes the action ; human action recognition ; 	<material> <task> <task> <otherscientificterm>	3 1 3 ; 3 3 3 ; 0 1 2 ; 1 1 3 ; 3 1 1 ; 3 2 3 ; 3 6 3	human action recognition from <material_0> has been intensively studied and attracting an increasing attention . online action detection goes one step further and is more challenging , which identifies the action type and <task_1> positions on the fly from the untrimmed stream . in this paper , we study the problem of online action detection from the streaming skeleton data . we propose a multi-task end-to-end joint classification-regression recurrent neural network to better explore the action type and temporal localiza-tion information . by employing a joint classification and regression optimization objective , this network is capable of automatically localizing the start and end points of actions more accurately . specifically , by leveraging the merits of the deep long short-term memory -lrb- lstm -rrb- subnetwork , the proposed model automatically captures the complex long-range temporal dynamics , which naturally avoids the typical sliding window design and thus ensures high computational efficiency . furthermore , the subtask of regression optimization provides the ability to forecast the action prior to its occurrence . to evaluate our proposed model , we build a large streaming video dataset with annotations . experimental results on our dataset and the public g3d dataset both demonstrate very promising performance of our scheme .
	machine translation evaluation ; sentence-level semantic equivalence classification ; . ; 	<task> <task> <metric> <metric>	2 1 3 ; 3 1 3 ; 3 2 3 ; 3 4 3 ; 0 6 1 ; 3 5 2 ; 3 6 3	the task of <task_0> is closely related to the task of <task_1> . this paper investigates the utility of applying standard mt evaluation methods -lrb- bleu , nist , wer and per -rrb- to building classifiers to predict semantic equivalence and entailment <metric_2> we also introduce a novel classification method based on per which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence <metric_2> our results show that mt evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment <metric_2> our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .
	black-box measure of similarity ; numerical optimization problem ; visual object tracking ; object model ; 	<metric> <task> <task> <method> <method>	4 0 4 ; 4 1 4 ; 4 3 4 ; 4 5 4 ; 1 1 2 ; 4 6 4 ; 4 2 4 ; 4 4 4	given an <method_3> and a <metric_0> between the <method_3> and candidate targets , we consider <task_2> as a <task_1> . during normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale . however , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the kalman filter do not provide a good estimate of object parameters in future frames . to recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve it via adaptive simulated annealing -lrb- asa -rrb- , a method that avoids becoming trapped at local modes and is much faster than exhaustive search . as a monte carlo approach , asa stochastically samples the parameter space , in contrast to local deterministic search . we apply cluster analysis on the sampled parameter space to redetect the object and renew the local tracker . our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with heavy occlusion and large camera motions . our approach outperforms state-of-the-art trackers on the vivid benchmark datasets .
	hand-crafted template-based or rule-based approaches ; automatically training modules ; quality of utterances ; natural language generator ; trainable components ;  ; utterances	<method> <method> <metric> <method> <method> <method> <otherscientificterm>	1 0 3 ; 6 2 4 ; 5 2 5 ; 5 6 5 ; 6 2 0 ; 5 4 5 ; 5 1 5 ; 4 4 0	techniques for <method_1> of a <method_3> have recently been proposed , but a fundamental concern is whether the <metric_2> produced with <method_4> can compete with <method_0> . in this paper we experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments . in order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two rule-based sentence planners , and two baseline sentence planners . we show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .
	novel view generation ; one-to-one teleconferencing applications ; position -lrb- ; eye contact ; , the ;  ; -rrb- ; images	<task> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm>	3 1 6 ; 7 1 4 ; 5 2 5 ; 5 6 5 ; 0 1 1 ; 5 3 3 ; 5 1 5	a new algorithm is proposed for <task_0> in <task_1> . given the video streams acquired by two cameras placed on either side of a computer monitor <material_4> proposed algorithm synthesises <otherscientificterm_7> from a virtual camera in arbitrary <otherscientificterm_2> typically located within the monitor <material_6> to facilitate <otherscientificterm_3> . our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation . the two main contributions of this paper are : i <material_6> a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii <material_6> a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface . furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -lrb- flicker <material_6> ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space . examples are given that demonstrate the robustness of the new algorithm to spatial and temporal artefacts for long stereo video streams . these include demonstrations of synthesis of cyclopean views of extended conversational sequences . we further demonstrate synthesis from a freely translating virtual camera .
	manual acquisition of semantic constraints ; 	<task> <otherscientificterm>	1 1 1 ; 1 6 1	manual acquisition of semantic constraints in broad domains is very expensive . this paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus . to a large extent , these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities . the scheme was implemented by gathering statistics on the output of other linguistic tools . an experiment was performed to resolve references of the pronoun `` it '' in sentences that were randomly selected from the corpus . the results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool .
	comparable , non-parallel corpora ; or not they ; discovering parallel sentences	<material> <method> <task>	0 1 2	we present a novel method for <task_2> in <material_0> . we train a maximum entropy classifier that , given a pair of sentences , can reliably determine whether <method_1> are translations of each other . using this approach , we extract parallel data from large chinese , arabic , and english non-parallel newspaper corpora . we evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system . we also show that a good-quality mt system can be built from scratch by starting with a very small parallel corpus -lrb- 100,000 words -rrb- and exploiting a large non-parallel corpus . thus , our method can be applied with great benefit to language pairs for which only scarce resources are available .
	statistical machine translation ; dynamic programming ; an efficient search ; in order	<task> <method> <method> <method>	1 1 0	in this paper , we describe a search procedure for <task_0> based on <method_1> . starting from a dp-based solution to the traveling salesman problem , we present a novel technique to restrict the possible word reordering between source and target language <method_3> to achieve <method_2> algorithm . a search restriction especially useful for the translation direction from german to english is presented . the experimental tests are carried out on the verbmobil task -lrb- german-english , 8000-word vocabulary -rrb- , which is a limited-domain spoken-language task .
	correctness proof ; previous invocations ; lr-parsers	<method> <method> <method>	0 6 2	a purely functional implementation of <method_2> is given , together with a simple <method_0> . it is presented as a generalization of the recursive descent parser . for non-lr grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of <method_1> . memo-functions also facilitate a simple way to construct a very compact representation of the parse forest . for lr -lrb- 0 -rrb- grammars , our algorithm is closely related to the recursive ascent parsers recently discovered by kruse-man aretz -lsb- 1 -rsb- and roberts -lsb- 2 -rsb- . extended cf grammars -lrb- grammars with regular expressions at the right hand side -rrb- can be parsed with a simple modification of the lr-parser for normal cf grammars .
	theory of discourse structure ; structure -rrb- ; -lrb- called ;  ; discourse	<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	3 1 3 ; 3 5 3 ; 3 6 3 ; 2 6 3	in this paper we explore a new <method_0> that stresses the role of purpose and processing in <otherscientificterm_4> . in this theory , <otherscientificterm_4> structure is composed of three separate but interrelated components : the structure of the sequence of utterances <otherscientificterm_2> the linguistic <otherscientificterm_1> , a structure of purposes <otherscientificterm_2> the intentional <otherscientificterm_1> , and the state of focus of attention <otherscientificterm_2> the attentional state -rrb- . the linguistic structure consists of segments of the <otherscientificterm_4> into which the utterances naturally aggregate . the intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them . the attentional state is an abstraction of the focus of attention of the participants as the <otherscientificterm_4> unfolds . the attentional state , being dynamic , records the objects , properties , and relations that are salient at each point of the <otherscientificterm_4> . the distinction among these components is essential to provide an adequate explanation of such <otherscientificterm_4> phenomena as cue phrases , referring expressions , and interruptions . the theory of attention , intention , and aggregation of utterances is illustrated in the paper with a number of example discourses . various properties of <otherscientificterm_4> are described , and explanations for the behaviour of cue phrases , referring expressions , and interruptions are explored . this theory provides a framework for describing the processing of utterances in a <otherscientificterm_4> . discourse processing requires recognizing how the utterances of the <otherscientificterm_4> aggregate into segments , recognizing the intentions expressed in the <otherscientificterm_4> and the relationships among intentions , and tracking the <otherscientificterm_4> through the operation of the mechanisms associated with attentional state . this processing description specifies in these recognition tasks the role of information from the <otherscientificterm_4> and from the participants ' knowledge of the domain .
	tree adjoining grammars ; head grammars ; grammatical formalisms	<method> <method> <method>	1 5 2 ; 0 5 2 ; 0 4 1	we examine the relationship between the two <method_2> : <method_0> and <method_1> . we briefly investigate the weak equivalence of the two formalisms . we then turn to a discussion comparing the linguistic expressiveness of the two formalisms .
	incorporate an adapted version of ; sentence-level and text-level anaphora ; dependency-based grammar model ; .	<task> <otherscientificterm> <method> <method>	2 1 1	we provide a unified account of <otherscientificterm_1> within the framework of a <method_2> . criteria for anaphora resolution within sentence boundaries rephrase major concepts from gb 's binding theory , while those for text-level anaphora <task_0> a grosz-sidner-style focus model .
	natural language text ; text revision ; languages ;  ; coedition	<material> <otherscientificterm> <material> <otherscientificterm> <task>	3 1 3 ; 4 1 1 ; 0 1 4 ; 3 5 3 ; 3 6 3	coedition of a <material_0> and its representation in some interlingual form seems the best and simplest way to share <otherscientificterm_1> across <material_2> . for various reasons , unl graphs are the best candidates in this context . we are developing a prototype where , in the simplest sharing scenario , naive users interact directly with the text in their language -lrb- l0 -rrb- , and indirectly with the associated graph . the modified graph is then sent to the unl-l0 deconverter and the result shown . if is is satisfactory , the errors were probably due to the graph , not to the deconverter , and the graph is sent to deconverters in other <material_2> . versions in some other <material_2> known by the user may be displayed , so that improvement sharing is visible and encouraging . as new versions are added with appropriate tags and attributes in the original multilingual document , nothing is ever lost , and cooperative working on a document is rendered feasible . on the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a lo-english or better a l0-unl dictionary , a morphosyntactic parser of l0 , and a canonical graph2tree transformation . establishing a `` best '' correspondence between the '' unl-tree + l0 '' and the '' ms-l0 structure '' , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible . a central goal of this research is to merge approaches from pivot mt , interactive mt , and multilingual text authoring .
	analogies between words ; 	<task> <task>	1 2 1	the reality of <task_0> is refuted by noone -lrb- e.g. , i walked is to to walk as i laughed is to to laugh , noted i walked : to walk : : i laughed : to laugh -rrb- . but computational linguists seem to be quite dubious about analogies between sentences : they would not be enough numerous to be of any use . we report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains . we give two estimates , a lower one and a higher one . as an analogy must be valid on the level of form as well as on the level of meaning , we relied on the idea that translation should preserve meaning to test for similar meanings .
	weighted rank loss ; mahalanobis distance ; weighted sum ;  ; ranks ; precision	<otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>	3 1 3 ; 3 3 3 ; 0 1 1 ; 3 2 3 ; 2 3 5 ; 3 5 3 ; 3 6 3	our goal is to learn a <task_1> by minimizing a <otherscientificterm_0> defined on the <otherscientificterm_2> of the <metric_5> at different <otherscientificterm_4> . our core motivation is that minimizing a <otherscientificterm_0> is a natural criterion for many problems in computer vision such as person re-identification . we propose a novel metric learning formulation called weighted approximate rank component analysis -lrb- warca -rrb- . we then derive a scalable stochastic gradient descent algorithm for the resulting learning problem . we also derive an efficient non-linear extension of warca by using the kernel trick . kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features . we also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently . we validate this new method on nine standard person re-identification datasets including two large scale market-1501 and cuhk03 datasets and show that we improve upon the current state-of-the-art methods on all of them .
	language model adaptation methods ; word list ; from the ; raw corpus ; 	<method> <otherscientificterm> <material> <material> <otherscientificterm>	4 1 4 ; 1 6 3 ; 1 1 0 ; 3 1 0 ; 4 2 4	in this paper , we discuss <method_0> given a <otherscientificterm_1> and a <material_3> . in this situation , the general method is to segment the <material_3> automatically using a <otherscientificterm_1> , correct the output sentences by hand , and build a model <material_2> segmented corpus . in this sentence-by-sentence error correction method , however , the annotator encounters grammatically complicated positions and this results in a decrease of productivity . in this paper , we propose to concentrate on correcting the positions in which the words in the list appear by taking a word as a correction unit . this method allows us to avoid these problems and go directly to capturing the statistical behavior of specific words in the application . in the experiments , we used a variety of methods for preparing a segmented corpus and compared the language models by their speech recognition accuracies . the results showed the advantages of our method .
	are all commonly modeled with ; multinomial or categorical distributions ; discrete data ; modeling problems ; 	<material> <method> <material> <task> <material>	4 0 4 ; 2 1 3 ; 4 1 4 ; 4 3 4 ; 4 1 0 ; 1 1 3	many practical <task_3> involve <material_2> that are best represented as draws from <method_1> . for example , nucleotides in a dna sequence , children 's names in a given state and year , and text documents <material_0> multinomial distributions . in all of these cases , we expect some form of dependency between the draws : the nucleotide at one position in the dna strand may depend on the preceding nucleotides , children 's names are highly correlated from year to year , and topics in text may be correlated and dynamic . these dependencies are not naturally captured by the typical dirichlet-multinomial formulation . here , we leverage a logistic stick-breaking representation and recent innovations in pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .
	robust operator ; minpran	<method> <method>	1 5 0	minpran , a new <method_0> , nds good ts in data sets where more than 50 % of the points are outliers . unlike other techniques that handle large outlier percentages , <method_1> does not rely on a known error bound for the good data . instead it assumes that the bad data are randomly -lrb- uniformly -rrb- distributed within the dynamic range of the sensor . based on this , <method_1> uses random sampling to search for the t and the number of inliers to the t that are least likely to have occurred randomly . it runs in time o -lrb- n 2 + sn log n -rrb- , where s is the number of random samples and n is the number of data points . we demonstrate analytically that <method_1> distinguishes good ts from ts to random data , and that <method_1> nds accurate ts and nearly the correct number of inliers , regardless of the percentage of true inliers . <method_1> 's properties are connrmed experimentally on synthetic data and compare favorably to least median of squares . related work applies <method_1> to complex range and intensity data 23 -rsb- .
	metarules -lrb- mps grammars -rrb- ; context-free phrase structure rules ; syntax of natural languages ; metagrammatical formalisms ; 	<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>	0 0 3 ; 4 2 4 ; 1 6 0 ; 1 0 3	metagrammatical formalisms that combine <otherscientificterm_1> and <otherscientificterm_0> allow concise statement of generalizations about the <otherscientificterm_2> . unconstrained mps grammars , unfortunately , are not computationally safe . we evaluate several proposals for constraining them , basing our assessment on computational tractability and explanatory adequacy . we show that none of them satisfies both criteria , and suggest new directions for research on alternative metagrammatical formalisms .
	automatic translation of natural language ; tree-adjoining grammars ; semantic interpretation ;  ; syntax	<task> <method> <task> <method> <otherscientificterm>	1 1 0 ; 3 1 3 ; 2 6 0 ; 1 1 2	the unique properties of <method_1> present a challenge for the application of <method_1> beyond the limited confines of <otherscientificterm_4> , for instance , to the task of <task_2> or <task_0> . we present a variant of <method_1> , called synchronous <method_1> , which characterize correspondences between languages . the formalism 's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow <method_1> to be used beyond their role in <otherscientificterm_4> proper . we discuss the application of synchronous <method_1> to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .
	on-line cursive handwriting analysis and recognition ; the line of ; model-based approach ; by two ; 	<task> <task> <method> <task> <otherscientificterm>	4 6 4 ; 3 0 1 ; 4 1 4 ; 2 1 0	a <method_2> to <task_0> is presented and evaluated . in this model , on-line handwriting is considered as a modulation of a simple cycloidal pen motion , described <task_3> coupled oscillations with a constant linear drift along <task_1> the writing . by slow modulations of the amplitudes and phase lags of the two oscillators , a general pen trajectory can be efficiently encoded . these parameters are then quantized into a small number of values without altering the writing intelligibility . a general procedure for the estimation and quantization of these cycloidal motion parameters for arbitrary handwriting is presented . the result is a discrete motor control representation of the continuous pen motion , via the quantized levels of the model parameters . this motor control representation enables successful word spotting and matching of cursive scripts . our experiments clearly indicate the potential of this dynamic representation for complete cursive handwriting recognition .
	categorization of objects ; estimating object pose ; object recognition task ; pose information ; view-invariant representation ; 	<task> <task> <task> <otherscientificterm> <method> <method>	5 0 5 ; 0 0 2 ; 4 1 0 ; 5 6 5 ; 5 4 5 ; 1 0 2 ; 0 6 1 ; 5 1 5	in the <task_2> , there exists a di-chotomy between the <task_0> and <task_1> , where the <task_0> necessitates a <method_4> , while the <task_1> requires a representation capable of capturing <otherscientificterm_3> over different categories of objects . with the rise of deep archi-tectures , the prime focus has been on object category recognition . deep learning methods have achieved wide success in this task . in contrast , object pose estimation using these approaches has received relatively less attention . in this work , we study how convolutional neural networks -lrb- cnn -rrb- architectures can be adapted to the task of simultaneous object recognition and pose estimation . we investigate and analyze the layers of various cnn models and extensively compare between them with the goal of discovering how the layers of distributed representations within cnns represent object <otherscientificterm_3> and how this contradicts with object category representations . we extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art .
	legal text hierarchy ; english-chinese bitexts ; numbering system ; 	<otherscientificterm> <material> <method> <material>	3 1 3	in this paper we present our recent work on harvesting <material_1> of the laws of hong kong from the web and aligning <material_1> to the subparagraph level via utilizing the <method_2> in the <otherscientificterm_0> . basic methodology and practical techniques are reported in detail . the resultant bilingual corpus , 10.4 m english words and 18.3 m chinese characters , is an authoritative and comprehensive text collection covering the specific and special domain of hk laws . it is particularly valuable to empirical mt research . this piece of work has also laid a foundation for exploring and harvesting <material_1> in a larger volume from the web .
	densely sampled rays ; scene description ; light fields ; matching . ; image-based representations ; 	<otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm>	5 2 5 ; 5 6 5 ; 3 1 5 ; 5 4 5 ; 5 3 3 ; 5 1 5	light fields are <method_4> that use <otherscientificterm_0> as a <otherscientificterm_1> . in this paper , we explore geometric structures of 3d lines in ray space for improving light field triangulation and stereo <task_3> the triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points -lrb- rays -rrb- . such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution . we show that the light field space is largely bi-linear due to 3d line segments in the scene , and direct tri-angulation of these bilinear subspaces leads to large errors . we instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply constrained delaunay triangulation -lrb- cdt -rrb- . based on our analysis , we further develop a novel line-assisted graph-cut -lrb- lagc -rrb- algorithm that effectively encodes 3d line constraints into light field stereo <task_3> experiments on synthetic and real data show that both our triangulation and lagc algorithms outperform state-of-the-art solutions in accuracy and visual quality .
	phrase-based statistical machine translation method ; non-contiguous phrases	<method> <material>	1 1 0	this paper presents a <method_0> , based on <material_1> , i.e. phrases with gaps . a method for producing such phrases from a word-aligned corpora is proposed . a statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the nist evaluation metric . translations are produced by means of a beam-search decoder . experimental results are presented , that demonstrate how the proposed method allows to better generalize from the training data .
	reading and learning ; glosser	<task> <method>	1 1 0	glosser is designed to support <task_0> to read in a foreign language . there are four language pairs currently supported by <method_1> : english-bulgarian , english-estonian , english-hungarian and french-dutch . the program is operational on unix and windows '95 platforms , and has undergone a pilot user-study . a demonstration -lrb- in unix -rrb- for applied natural language processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -lrb- icall -rrb- , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .
	fine-grained modeling of unknown word features ; priors in conditional loglinear models ; multiple consecutive words ; dependency network representation ; part-of-speech tagger ; tag contexts ; lexical features ; 	<method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method>	1 1 4 ; 5 1 4 ; 0 1 4 ; 3 1 5 ; 6 1 4 ; 7 2 7	we present a new <method_4> that demonstrates the following ideas : -lrb- i -rrb- explicit use of both preceding and following <otherscientificterm_5> via a <method_3> , -lrb- ii -rrb- broad use of <otherscientificterm_6> , including jointly conditioning on <otherscientificterm_2> , -lrb- iii -rrb- effective use of <otherscientificterm_1> , and -lrb- iv -rrb- <method_0> . using these ideas together , the resulting tagger gives a 97.24 % accuracy on the penn treebank wsj , an error reduction of 4.4 % on the best previous single automatically learned tagging result .
	camera view ; feature extraction ; cnn -rrb- ; person re-identification ;  ; pose ; illumination ; occlusion	<otherscientificterm> <method> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	4 3 1 ; 4 1 4 ; 4 3 4 ; 4 4 4 ; 4 6 4 ; 1 1 2	person re-identification is challenging due to the large variations of <otherscientificterm_5> , <otherscientificterm_6> , <otherscientificterm_7> and <otherscientificterm_0> . owing to these variations , the pedestrian data is distributed as highly-curved manifolds in the feature space , despite the current convolutional neural networks -lrb- <material_2> 's capability of <method_1> . however , the distribution is unknown , so it is difficult to use the geodesic distance when comparing two samples . in practice , the current deep embedding methods use the euclidean distance for the training and test . on the other hand , the manifold learning methods suggest to use the euclidean distance in the local range , combining with the graphical relationship between samples , for approximating the geodesic distance . from this point of view , selecting suitable positive -lrb- i.e. intra-class -rrb- training samples within a local range is critical for training the cnn embedding , especially when the data has large intra-class variations . in this paper , we propose a novel moderate positive sample mining method to train robust cnn for person re-identification , dealing with the problem of large variation . in addition , we improve the learning by a metric weight constraint , so that the learned metric has a better generalization ability . experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification . therefore , the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification .
	automatic speech recognition system ; utterance verification ; out-of-vocabulary words ; spontaneous speech ; acoustic noises ; 	<method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>	1 5 0 ; 5 2 5 ; 5 6 5 ; 5 3 5 ; 5 5 5 ; 5 1 5	utterance verification -lrb- uv -rrb- is a critical function of an <method_0> working on real applications where <otherscientificterm_3> , <otherscientificterm_2> and <otherscientificterm_4> are present . in this paper we present a new uv procedure with two major features : a -rrb- confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent oov words and noises . thus the asr system is designed to deal with what we refer to as word spotting and noise spotting capabilities . b -rrb- the uv procedure is based on three different confidence tests , two based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure . experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .
	automatic speech recognition -lrb- ; discrete nerve-action potentials ; analog pressure wave ; used visual ; -rsb- . ; neuronal processing ; al. -lsb- ; encoding sound ; 	<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <task> <method>	8 2 8 ; 6 6 4 ; 6 1 8 ; 7 1 5 ; 8 1 8 ; 3 1 0 ; 8 4 8 ; 4 1 8 ; 1 1 2	a critical step in <task_7> for <task_5> occurs when the <otherscientificterm_2> is coded into <otherscientificterm_1> . recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we <method_3> inspection and <otherscientificterm_0> asr -rrb- to investigate an offset adaptation -lrb- oa -rrb- model proposed by zhang et <method_6> 1 <method_4> oa improved phase locking in the auditory nerve -lrb- an -rrb- and raised asr accuracy for features derived from an fibers -lrb- anfs -rrb- . we also found that oa is crucial for auditory processing by onset neurons -lrb- ons -rrb- in the next neuronal stage , the auditory brainstem . multi-layer perceptrons -lrb- mlps -rrb- performed much better than standard gaussian mixture models -lrb- gmms -rrb- for both our anf-based and on-based auditory features . similar results were previously obtained with msg -lrb- modulation-filtered spec-trogram -rrb- auditory features -lsb- 2 <method_4> thus we believe researchers working with novel features should consider trying mlps .
	large datasets ; required . ; high-capacity models ; computer vision ; 	<material> <material> <method> <task> <otherscientificterm>	4 3 1 ; 4 0 4 ; 2 1 3 ; 4 1 4 ; 0 1 2	recent progress in <task_3> has been driven by <method_2> trained on <material_0> . unfortunately , creating <material_0> with pixel-level labels has been extremely costly due to the amount of human effort <material_1> in this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games .
	unsupervised seg-mentation of whole objects ; soft , binary mattes ; partial scene segmentation	<task> <otherscientificterm> <task>	1 1 2 ; 2 1 0	we propose a novel step toward the <task_0> by combining '' hints '' of <task_2> offered by multiple <otherscientificterm_1> . these mattes are implied by a set of hypothesized object boundary fragments in the scene . rather than trying to find or define a single '' best '' segmentation , we generate multiple segmentations of an image . this reflects contemporary methods for unsupervised object discovery from groups of images , and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects . our proposed approach builds on recent advances in spectral clustering , image matting , and boundary detection . it is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in unsupervised object discovery without top-down knowledge .
	language resource quality ;  ; nlp ; translations	<metric> <task> <task> <task>	1 6 1 ; 1 1 1 ; 0 3 2 ; 1 2 1 ; 1 5 3	language resource quality is crucial in <task_2> . many of the resources used are derived from data created by human beings out of an <task_2> context , especially regarding mt and reference <task_3> . indeed , automatic evaluations need high-quality data that allow the comparison of both automatic and human <task_3> . the validation of these resources is widely recommended before being used . this paper describes the impact of using different-quality references on evaluation . surprisingly enough , similar scores are obtained in many cases regardless of the quality . thus , the limitations of the automatic metrics used within mt are also discussed in this regard .
	full scale two-level morphological description ; turkish word structures ; 	<task> <material> <method>	2 5 2 ; 0 1 1 ; 2 3 2 ; 2 0 2 ; 2 1 2	this poster paper describes a <task_0> -lrb- karttunen , 1983 ; koskenniemi , 1983 -rrb- of <material_1> . the description has been implemented using the pc-kimmo environment -lrb- antworth , 1990 -rrb- and is based on a root word lexicon of about 23,000 roots words . almost all the special cases of and exceptions to phonological and morphological rules have been implemented . turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words . turkish has finite-state but nevertheless rather complex morphotactics . morphemes added to a root word or a stem can convert the word from a nominal to a verbal structure or vice-versa , or can create adverbial constructs . the surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony .
	fundamental frequency contour of speech ; text-to-speech synthesis ; text input ; 	<otherscientificterm> <task> <material> <method>	3 3 3 ; 3 1 3 ; 0 1 1 ; 2 1 0	this paper deals with the problem of generating the <otherscientificterm_0> from a <material_2> for <task_1> . we have previously introduced a statistical model describing the generating process of speech f0 contours , based on the discrete-time version of the fujisaki model . one remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the fujisaki-model parameters from raw f0 contours . to associate a sequence of the fujisaki-model parameters with a <material_2> based on statistical learning , this paper proposes extending this model to a context-dependent one . we further propose a parameter training algorithm for the present model based on a decision tree-based context clustering .
	evaluation of object detection cascades ; space of candidate regions ; divide-and-conquer procedure ; the search ; 	<task> <method> <method> <method> <task>	4 0 4 ; 3 4 4 ; 4 1 4 ; 1 3 2 ; 3 1 4 ; 4 5 4 ; 4 2 4 ; 4 4 4	we introduce a method to accelerate the <task_0> with the help of a <method_2> in the <method_1> . compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up <method_3> . furthermore , we show how the recently developed efficient subwindow search -lrb- ess -rrb- procedure -lsb- 11 -rsb- can be integrated into the last stage of our method . this allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions , in particular kernel-ized support vector machines . experiments on the pascal voc 2006 dataset show an acceleration of more than 50 % by our method compared to standard cascade evaluation .
	background modeling ; vision systems ; 	<task> <task> <otherscientificterm>	0 0 1 ; 2 3 2 ; 2 1 2	background modeling is an important component of many <task_1> . existing work in the area has mostly addressed scenes that consist of static or quasi-static structures . when the scene exhibits a persistent dynamic behavior in time , such an assumption is violated and detection performance deteriorates . in this paper , we propose a new method for the modeling and subtraction of such scenes . towards the modeling of the dynamic characteristics , optical flow is computed and utilized as a feature in a higher dimensional space . inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for density estimation using kernels . extensive experiments demonstrate the utility and performance of the proposed approach .
	massive , possibly multilingual , audio and textual document sources ; information distillation ; 	<material> <task> <otherscientificterm>	2 1 2	information distillation aims to extract relevant pieces of information related to a given query from <material_0> . in this paper , we present our approach for using information extraction annotations to augment document retrieval for distillation . we take advantage of the fact that some of the distillation queries can be associated with annotation elements introduced for the nist automatic content extraction -lrb- ace -rrb- task . we experimentally show that using the ace events to constrain the document set returned by an information retrieval engine significantly improves the precision at various recall rates for two different query templates .
	an arbitrary viewpoint . ; affine-invariant image patches ; models from ; spatial relationships ; three-dimensional objects ; reconstruction , ;  ; single ; in	<task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <task> <task>	8 6 7 ; 2 1 7 ; 6 1 6 ; 6 1 0 ; 1 3 4 ; 3 3 1 ; 2 1 8 ; 5 6 2 ; 5 1 8 ; 5 1 7	this paper presents a novel representation for <otherscientificterm_4> in terms of <otherscientificterm_1> and their <otherscientificterm_3> . multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and <otherscientificterm_5> allowing the acquisition of true three-dimensional affine and euclidean <method_2> multiple images and their recognition <task_8> a <task_7> photograph taken from <task_0> the proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes . preliminary modeling and recognition results are presented .
	nearest neighbor search ;  ; distance	<task> <otherscientificterm> <otherscientificterm>	1 4 1 ; 1 1 1 ; 1 5 1 ; 1 6 1	fast algorithms for <task_0> have in large part focused on 2 <otherscientificterm_2> . here we develop an approach for 1 <otherscientificterm_2> that begins with an explicit and exactly distance-preserving embedding of the points into 2 2 . we show how this can efficiently be combined with random-projection based methods for 2 nn search , such as locality-sensitive hashing -lrb- lsh -rrb- or random projection trees . we rigorously establish the correctness of the methodology and show by experimentation using lsh that it is competitive in practice with available alternatives .
