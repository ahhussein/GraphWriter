	computing optical flow , shape , motion , lighting , and albedo ; rigidly-moving lambertian object ; image sequence ; distant illumination	<task> <material> <material> <otherscientificterm>	1 0 2 ; 3 0 1	this paper presents an algorithm for <task_0> from an <material_2> of a <material_1> under <otherscientificterm_3> . the problem is formulated in a manner that subsumes structure from motion , multi-view stereo , and photo-metric stereo as special cases . the algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces . the algorithm works by iteratively estimating affine camera parameters , illumination , shape , and albedo in an alternating fashion . results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera .
	entity-oriented approach ; restricted-domain parsing ; together . ; 	<method> <task> <otherscientificterm> <method>	0 1 1 ; 3 1 3 ; 3 2 3	an <method_0> to <task_1> is proposed . in this approach , the definitions of the structure and surface representation of domain entities are grouped <otherscientificterm_2> like semantic grammar , this allows easy exploitation of limited domain semantics . in addition , it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input . several advantages from the point of view of language definition are also noted . representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses . a parser incorporating the control structure and the parsing strategies is currently under implementation .
	formalism of category cooccurrence restrictions ; category cooccurrence restrictions ; in other ; parsing algorithms ;  ; be	<task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>	4 1 4 ; 4 0 4	this paper summarizes the <task_0> and describes two <method_3> that interpret it . ccrs are boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which can not <otherscientificterm_5> captured <otherscientificterm_2> current syntax formalisms . the use of ccrs leads to syntactic descriptions formulated entirely with restrictive statements . the paper shows how conventional algorithms for the analysis of context free languages can <otherscientificterm_5> adapted to the ccr formalism . special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .
	text mining method ; distributional hypothesis ; synonymous expressions	<method> <otherscientificterm> <otherscientificterm>	1 1 0 ; 0 1 2	we present a <method_0> for finding <otherscientificterm_2> based on the <otherscientificterm_1> in a set of coherent corpora . this paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus . our approach is based on the idea that one person tends to use one expression for one meaning . according to our assumption , most of the words with similar context features in each author 's corpus tend not to be <otherscientificterm_2> . our proposed method improves the accuracy of our term aggregation system , showing that our approach is successful .
	inherent uncertainty of the estimation procedure ; efficient robust estimation algorithm ; robust estimation ; 	<otherscientificterm> <method> <task> <task>	3 1 3 ; 3 3 3 ; 3 4 3	in this work , we present a technique for <task_2> , which by explicitly incorporating the <otherscientificterm_0> , results in a more <method_1> . in addition , we build on recent work in randomized model verification , and use this to characterize the ` non-randomness ' of a solution . the combination of these two strategies results in a <task_2> procedure that provides a significant speed-up over existing ransac techniques , while requiring no prior information to guide the sampling process . in particular , our algorithm requires , on average , 3-10 times fewer samples than standard ransac , which is in close agreement with theoretical predictions . the efficiency of the algorithm is demonstrated on a selection of geometric estimation problems .
	augmented transition network ; dialog model ; about task-oriented ; a device ; of verbal ; 	<method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>	5 0 5 ; 4 1 3 ; 4 1 0 ; 5 1 5 ; 0 5 1 ; 5 6 5	an attempt has been made to use an <method_0> as a procedural <method_1> . the development of such a model appears to be important in several respects : as <method_3> to represent and to use different dialog schemata proposed in empirical conversation analysis ; as <method_3> to represent and to use models <otherscientificterm_4> interaction ; as <method_3> combining knowledge about dialog schemata and about verbal interaction with knowledge <otherscientificterm_2> and goal-directed dialogs . a standard atn should be further developed in order to account for the verbal interactions of task-oriented dialogs .
	an svm to separate ; question answering systems ; unsupervised learning method ; web search engines ; single-snippet answers ; 	<material> <method> <method> <method> <otherscientificterm> <material>	1 1 3 ; 2 1 4 ; 5 3 5 ; 0 1 5 ; 5 1 5 ; 5 2 5	we present a practically <method_2> to produce <otherscientificterm_4> to definition questions in <method_1> that supplement <method_3> . the method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train <material_0> the two classes . we show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better .
	classical decision-theoretic problem of weighted expert voting ; statistical learning perspective ; 	<task> <method> <otherscientificterm>	2 1 2 ; 1 1 0	we revisit the <task_0> from a <method_1> . in particular , we examine the consistency -lrb- both asymptotic and finitary -rrb- of the optimal nitzan-paroush weighted majority and related rules . in the case of known expert competence levels , we give sharp error estimates for the optimal rule . when the competence levels are unknown , they must be empirically estimated . we provide frequentist and bayesian analyses for this situation . some of our proof techniques are non-standard and may be of independent interest . the bounds we derive are nearly optimal , and several challenging open problems are posed . experimental results are provided to illustrate the theory .
	log partition function of a product distribution ; reweighted version of the kikuchi approximation ; to the kikuchi ; produce global ; the kikuchi ; region graph ; kikuchi approximation ;  ; . ; product	<task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm>	8 1 7 ; 9 0 2 ; 1 1 0 ; 7 0 7 ; 0 0 5	we analyze a <method_1> for estimating the <task_0> defined over a <otherscientificterm_5> . we establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in <otherscientificterm_4> expansion , and show that a reweighted version of the sum <otherscientificterm_9> algorithm applied <otherscientificterm_2> region graph will <otherscientificterm_3> optima of the <method_6> whenever the algorithm converges <method_8> when the <otherscientificterm_5> has two layers , corresponding to a bethe approximation , we show that our sufficient conditions for concavity are also necessary <method_8> finally , we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the <otherscientificterm_5> . we conclude with simulations that demonstrate the advantages of the reweighted kikuchi approach .
	decision tree based approach ; spoken dialogue ; pronoun resolution	<method> <task> <task>	2 1 1 ; 0 1 2	we apply a <method_0> to <task_2> in <task_1> . our system deals with pronouns with np - and non-np-antecedents . we present a set of features designed for <task_2> in <task_1> and determine the most promising features . we evaluate the system on twenty switchboard dialogues and show that it compares well to byron 's -lrb- 2002 -rrb- manually tuned system .
	classification stage ; class problem ; image ; orien-tations	<method> <task> <material> <otherscientificterm>	0 1 1	we present a new approach for building an efficient and robust <method_0> for the two <task_1> , that localizes objects that may appear in the <material_2> under different <otherscientificterm_3> . in contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an estimation stage and a <method_0> . the estimator yields an initial set of potential object poses that are then validated by the <method_0> . this methodology allows reducing the time complexity of the algorithm while classification results remain high . the <method_0> we use in both stages is based on a boosted combination of random ferns over local histograms of oriented gradients -lrb- hogs -rrb- , which we compute during a pre-processing step . both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time . we show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions .
	lincoln csr system ; 	<method> <method>	1 1 1 ; 1 4 1 ; 1 6 1	the following describes recent work on the <method_0> . some new variations in semiphone modeling have been tested . a very simple improved duration model has reduced the error rate by about 10 % in both triphone and semiphone systems . a new training strategy has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . finally , the recognizer has been modified to use bigram back-off language models . the system was then transferred from the rm task to the atis csr task and a limited number of development tests performed . evaluation test results are presented for both the rm and atis csr tasks .
	interactive machine translation ; 	<task> <method>	1 1 1	a new approach for <task_0> where the author interacts during the creation or the modification of the document is proposed . the explanation of an ambiguity or an error for the purposes of correction does not use any concepts of the underlying linguistic theory : it is a reformulation of the erroneous or ambiguous sentence . the interaction is limited to the analysis step of the translation process . this paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser 's multiple output . some examples of paraphrasing ambiguous sentences are presented .
	statistical machine translation ; syntactic information ; phrasal translation	<task> <otherscientificterm> <task>	1 6 2	we describe a novel approach to <task_0> that combines <otherscientificterm_1> in the source language with recent advances in <task_2> . this method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component . we align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model . we describe an efficient decoder and show that using these tree-based models in combination with conventional smt models provides a promising approach that incorporates the power of phrasal smt with the linguistic generality available in a parser .
	long-range temporal interactions ; visual cues ; appearance ; video ; motion	<otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm>	1 0 3 ; 2 5 1 ; 4 5 1 ; 4 6 2	video provides not only rich <otherscientificterm_1> such as <otherscientificterm_4> and <otherscientificterm_2> , but also much less explored <otherscientificterm_0> among objects . we aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition . motivated by this goal , we seek to obtain spatio-temporal over-segmentation of a video into regions that respect object boundaries and , at the same time , associate object pix-els over many video frames . the contributions of this paper are twofold . first , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates long-range <otherscientificterm_4> cues from the past and future frames in the form of clusters of point tracks with coherent <otherscientificterm_4> . second , we devise a new track clustering cost function that includes occlusion reasoning , in the form of depth ordering constraints , as well as <otherscientificterm_4> similarity along the tracks . we evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies .
	multiscale 2d feature detection and description algorithm ; nonlinear scale spaces ; kaze features ; 	<method> <otherscientificterm> <method> <otherscientificterm>	1 0 0 ; 2 5 0 ; 3 1 3 ; 3 0 3 ; 3 4 3 ; 3 3 3 ; 3 6 3	in this paper , we introduce <method_2> , a novel <method_0> in <otherscientificterm_1> . previous approaches detect and describe features at different scale levels by building or approximating the gaussian scale space of an image . however , gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise , reducing localization accuracy and distinctiveness . in contrast , we detect and describe 2d features in a nonlinear scale space by means of nonlinear diffusion filtering . in this way , we can make blurring locally adaptive to the image data , reducing noise but retaining object boundaries , obtaining superior localization accuracy and distinctiviness . the nonlinear scale space is built using efficient additive operator splitting -lrb- aos -rrb- techniques and variable con-ductance diffusion . we present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces . even though our features are somewhat more expensive to compute than surf due to the construction of the nonlinear scale space , but comparable to sift , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .
	semantic web documents ; 	<material> <task>	1 1 1 ; 1 4 1 ; 1 3 1 ; 1 6 1 ; 1 0 1	semantic web documents that encode facts about entities on the web have been growing rapidly in size and evolving over time . creating summaries on lengthy <material_0> for quick identification of the corresponding entity has been of great contemporary interest . in this paper , we explore automatic summa-rization techniques that characterize and enable identification of an entity and create summaries that are human friendly . specifically , we highlight the importance of diversified -lrb- faceted -rrb- summaries by combining three dimensions : diversity , uniqueness , and popularity . our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts , and picks representative facts from each group to form concise -lrb- i.e. , short -rrb- and comprehensive -lrb- i.e. , improved coverage through diversity -rrb- summaries . we evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization .
	fast computation of lexical affinity models ; parametric affinity ; 	<task> <otherscientificterm> <method>	2 6 2 ; 2 1 2 ; 2 4 2	we present a framework for the <task_0> . the framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a <otherscientificterm_1> model . in comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus . the framework is flexible , allowing fast adaptation to applications and it is scalable . we apply it in combination with a terabyte corpus to answer natural language tests , achieving encouraging results .
	categorizing unknown words ; class of ; 	<task> <method> <otherscientificterm>	2 6 2 ; 2 1 2 ; 2 4 2	this paper introduces a system for <task_0> . the system is based on a multi-component architecture where each component is responsible for identifying one <method_1> unknown words . the focus of this paper is the components that identify names and spelling errors . each component uses a decision tree architecture to combine multiple types of evidence about the unknown word . the system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words .
	cclinc -lrb- common coalition language system at lincoln laboratory -rrb- ; korean-to-english machine translation system ; frame .	<method> <method> <method>	0 5 1	at mit lincoln laboratory , we have been developing a korean-to-english machine translation system <method_0> . the cclinc korean-to-english translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic <method_2> the key features of the system include : -lrb- i -rrb- robust efficient parsing of korean -lrb- a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments -rrb- . -lrb- ii -rrb- high quality translation via word sense disambiguation and accurate word order generation of the target language . -lrb- iii -rrb- rapid system development and porting to new domains via knowledge-based automated acquisition of grammars . having been trained on korean newspaper articles on missiles and chemical biological warfare , the system produces the translation output sufficient for content understanding of the original document .
	open-domain question answering capability ; language processing modules ; planning-based architecture ; javelin system ; 	<task> <method> <method> <method> <method>	3 1 0 ; 2 2 3 ; 1 6 2 ; 1 2 3	the <method_3> integrates a flexible , <method_2> with a variety of <method_1> to provide an <task_0> on free text . the demonstration will focus on how javelin processes questions and retrieves the most likely answer candidates from the given text corpus . the operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session .
	online left to right chart-parser ; head-driven statistical parsing model ; simultaneous language model ; large-vocabulary speech recognition	<method> <method> <method> <task>	2 6 0 ; 1 1 2 ; 1 1 0 ; 2 1 3 ; 0 1 3	we present the first application of the <method_1> of collins -lrb- 1999 -rrb- as a <method_2> and <method_0> for <task_3> . the model is adapted to an <method_0> for word lattices , integrating acoustic , n-gram , and <method_0> probabilities . the <method_0> uses structural and lexical dependencies not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships . experiments on the wall street journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .
	image composition -lrb- or mosaicing -rrb- ; video analysis and representation ; 	<task> <task> <task>	0 2 1 ; 2 4 2 ; 2 1 2 ; 2 6 2 ; 2 3 2	image composition -lrb- or mosaicing -rrb- has attracted a growing attention in recent years as one of the main elements in <task_1> . in this paper we deal with the problem of global alignment and super-resolution . we also propose to evaluate the quality of the resulting mosaic by measuring the amount of blurring . global registration is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps . experimental comparison with other techniques shows the effectiveness of our approach .
	lexicon grammar for polish ; 	<method> <task>	1 1 1 ; 1 6 1 ; 1 5 1 ; 1 0 1	the project presented here is a part of a long term research program aiming at a full <method_0> . the main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in polish . we present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description . in this paper we focus on the results of the third phase . the presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for polish . in the paper we describe the syntlex dictionary of collocations and announce some future research intended to be a separate project continuation .
	hash-tag recommendation task ;  ; microblogs	<task> <otherscientificterm> <material>	1 1 1 ; 1 4 1 ; 1 3 1 ; 0 1 2 ; 1 0 1	along with the increasing requirements , the <task_0> for <material_2> has been receiving considerable attention in recent years . various researchers have studied the problem from different aspects . however , most of these methods usually need handcrafted features . motivated by the successful use of convolutional neural networks -lrb- cnns -rrb- for many natural language processing tasks , in this paper , we adopt cnns to perform the hashtag recommendation problem . to incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel architecture with an attention mechanism . the results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods . by incorporating trigger words into the consideration , the relative improvement of the proposed method over the state-of-the-art method is around 9.4 % in the f1-score .
	word sense disambiguation problems ; expectation-maximization algorithm ; text classification problems ; unsupervised learning method	<task> <method> <task> <method>	1 1 3 ; 1 1 2 ; 3 1 0	in this paper , we improve an <method_3> using the <method_1> proposed by nigam et al. for <task_2> in order to apply <method_3> to <task_0> . the improved method stops the em algorithm at the optimum iteration number . to estimate that number , we propose two methods . in experiments , we solved 50 noun wsd problems in the japanese dictionary task in senseval2 . the score of our method is a match for the best public score of this task . furthermore , our methods were confirmed to be effective also for verb wsd problems .
	dividing sentences in chunks of words ; information retrieval ; information extraction ;  ; parsing	<task> <task> <task> <method> <task>	2 6 1 ; 3 1 3 ; 4 6 2 ; 3 4 3 ; 0 1 4 ; 0 1 2 ; 0 1 1	dividing sentences in chunks of words is a useful preprocessing step for <task_4> , <task_2> and <task_1> . -lrb- ramshaw and marcus , 1995 -rrb- have introduced a `` convenient '' data representation for chunking by converting it to a tagging task . in this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . we will show that the data representation choice has a minor influence on chunking performance . however , equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set .
	model for answers and ; question answering system ; web . ; trained on ; million question/answer ; 	<material> <method> <method> <method> <method> <material>	3 1 4 ; 3 1 2	in this paper we describe and evaluate a <method_1> that goes beyond answering factoid questions . we focus on faq-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language <material_0> a transformation model for answer/question terms , <method_3> a corpus of 1 <method_4> pairs collected from the web .
	predictions of intelligibility ; measures of speech ; diverse noisy situations ; speech quality ; -rrb- . ; the perceptual ; synthesized speech ; 	<task> <metric> <otherscientificterm> <metric> <metric> <metric> <material> <metric>	7 5 5 ; 7 4 7 ; 6 1 0 ; 7 5 7 ; 7 6 7 ; 7 1 7 ; 7 3 7 ; 3 6 4 ; 4 5 5 ; 4 6 7 ; 7 2 7 ; 2 0 6 ; 3 5 5 ; 5 6 7 ; 1 4 0	in this paper we evaluate four objective <metric_1> with regards to <task_0> of <material_6> in <otherscientificterm_2> . we evaluated three intel-ligibility measures , the dau measure , the glimpse proportion and the speech intelligibility index -lrb- sii -rrb- and a quality measure , <metric_5> evaluation of <metric_3> -lrb- pesq <metric_4> for the generation of <material_6> we used a state of the art hmm-based speech synthesis system . the noisy conditions comprised four additive noises . the measures were compared with subjective intelligibility scores obtained in listening tests . the results show the dau and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores . all measures gave less accurate <task_0> for synthetic speech than have previously been found for natural speech ; in particular the sii measure . in additional experiments , we processed the <material_6> by an ideal binary mask before adding noise . the glimpse measure gave the most accurate intelligibility predictions in this situation .
	'' graphics for vision '' approach ; large and imperfect data set ; tensor voting ;  ; rod-tv ; reconstruction	<method> <material> <method> <method> <method> <task>	2 6 4 ; 3 2 3 ; 3 1 3 ; 2 1 5 ; 3 0 3 ; 3 5 3 ; 3 4 3 ; 0 1 5 ; 1 1 5 ; 3 6 3 ; 4 1 5	a <method_0> is proposed to address the problem of <task_5> from a <material_1> : <task_5> on demand by <method_2> , or <method_4> . <method_4> simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -lrb- lod -rrb- . locally inferred surface elements are robust to noise and better capture local shapes . by inferring per-vertex normals at sub-voxel precision on the fly , we can achieve interpolative shading . since these missing details can be recovered at the current level of detail , our result is not upper bounded by the scanning resolution . by relaxing the mesh connectivity requirement , we extend <method_4> and propose a simple but effective multiscale feature extraction algorithm . <method_4> consists of a hierarchical data structure that encodes different levels of detail . the local <task_5> algorithm is <method_2> . it is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood . we compare our approach and present encouraging results .
	colon , dash , ellipsis , exclamation ; , question mark ; chinese punctuation marks ; ellipsis , exclamation ; rhetorical structure ; discourse processing ;  ; punctuation ; . ; and	<task> <material> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>	9 5 3 ; 7 1 5 ; 9 6 8 ; 6 0 6 ; 6 1 6 ; 6 3 6 ; 8 5 3 ; 4 6 7 ; 6 6 6 ; 6 5 3 ; 3 2 1 ; 4 1 5 ; 8 6 6	both <otherscientificterm_4> and <otherscientificterm_7> have been helpful in <task_5> . based on a corpus annotation project , this paper reports the discursive usage of 6 <task_2> in news commentary texts : <task_0> mark <material_1> , <otherscientificterm_9> semicolon <otherscientificterm_8> the rhetorical patterns of these marks are compared against patterns around cue phrases in general <otherscientificterm_8> results show that these <task_2> , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , <otherscientificterm_9> can be used as distinctive indicators of nuclearity in chinese texts .
	markov random field models ; rotation of image textures ;  ; features	<method> <otherscientificterm> <method> <otherscientificterm>	2 6 2 ; 2 1 2 ; 2 0 2 ; 0 1 3	the <otherscientificterm_3> based on <method_0> are usually sensitive to the <otherscientificterm_1> . this paper develops an anisotropic circular gaussian mrf -lrb- acgmrf -rrb- model for modelling rotated image textures and retrieving rotation-invariant texture <otherscientificterm_3> . to overcome the singularity problem of the least squares estimate -lrb- lse -rrb- method , an approximate least squares estimate -lrb- alse -rrb- method is proposed to estimate the parameters of the acgmrf model . the rotation-invariant <otherscientificterm_3> can be obtained from the parameters of the acgmrf model by the one-dimensional -lrb- 1-d -rrb- discrete fourier transform -lrb- dft -rrb- . significantly improved accuracy can be achieved by applying the rotation-invariant <otherscientificterm_3> to classify sar -lrb- synthetic aperture radar -rrb- sea ice and brodatz imagery .
	label sequence models ; semantic role labeling ; viterbi decoding ; independent classifiers ; 	<method> <task> <method> <method> <otherscientificterm>	4 4 4 ; 3 6 0 ; 4 2 4 ; 3 1 1 ; 2 1 0 ; 4 3 4	despite much recent progress on accurate <task_1> , previous work has largely used <method_3> , possibly combined with separate <method_0> via <method_2> . this stands in stark contrast to the linguistic observation that a core argument frame is a joint structure , with strong dependencies between arguments . we show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models . this system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on propbank .
	determines the likelihood ; generation ; ambiguity ; . ; scoring	<method> <task> <otherscientificterm> <method> <otherscientificterm>	3 1 0 ; 0 1 4	one of the major problems one is faced with when decomposing words into their constituent parts is <otherscientificterm_2> : the <task_1> of multiple analyses for one input word , many of which are implausible <method_3> in order to deal with <otherscientificterm_2> , the morphological parser morpa is provided with a probabilistic context-free grammar -lrb- pcfg -rrb- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based <otherscientificterm_4> function which <method_0> of each successful parse <method_3> consequently , remaining analyses can be ordered along a scale of plausibility <method_3> test performance data will show that a pcfg yields good results in morphological parsing <method_3> morpa is a fully implemented parser developed for use in a text-to-speech conversion system .
	korean phonology structure grammar ; korean phonological knowledge base system ; unification-based grammar formalism	<method> <task> <method>	0 5 2 ; 2 1 1	this paper describes the framework of a <task_1> using the <method_2> : <method_0> . the approach of kpsg provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system . we show that the proposed approach is more describable than other approaches such as those employing a traditional generative phonological approach .
	search tool ; ngrams	<method> <otherscientificterm>	0 1 1	in this paper , we will describe a <method_0> for a huge set of <otherscientificterm_1> . the tool supports queries with an arbitrary number of wildcards . it takes a fraction of a second for a search , and can provide the fillers of the wildcards . the system runs on a single linux pc with reasonable size memory -lrb- less than 4gb -rrb- and disk space -lrb- less than 400gb -rrb- . this system can be a very useful tool for linguistic knowledge discovery and other nlp tasks .
	intelligent interactive systems ; 	<method> <task>	1 1 1 ; 1 2 1	for <method_0> to communicate with humans in a natural manner , they must have knowledge about the system users . this paper explores the role of user modeling in such systems . it begins with a characterization of what a user model is and how it can be used . the types of information that a user model may be required to keep about a user are then identified and discussed . user models themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic . next , the benefits and costs of implementing a user modeling component for a system are weighed in light of several aspects of the interaction requirements that may be imposed by the system . finally , the current state of research in user modeling is summarized , and future research topics that must be addressed in order to achieve powerful , general user modeling systems are assessed .
	unstructured data sources ; information extraction techniques ; structured databases ; newswire documents ; web	<material> <method> <material> <material> <material>	3 5 0 ; 1 1 2 ; 4 5 0 ; 0 1 1 ; 4 6 3	information extraction techniques automatically create <material_2> from <material_0> , such as the <material_4> or <material_3> . despite the successes of these systems , accuracy will always be imperfect . for many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . the information extraction system we evaluate is based on a linear-chain conditional random field -lrb- crf -rrb- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a markov model . we implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .
	information redundancy in multilingual input ; machine translation ; multilingual summaries ; 	<otherscientificterm> <task> <task> <task>	0 1 1 ; 3 1 3 ; 0 1 2	in this paper , we use the <otherscientificterm_0> to correct errors in <task_1> and thus improve the quality of <task_2> . we consider the case of multi-document summarization , where the input documents are in arabic , and the output summary is in english . typically , information that makes it to a summary appears in many different lexical-syntactic forms in the input documents . further , the use of multiple <task_1> systems provides yet more redundancy , yielding different ways to realize that information in english . we demonstrate how errors in the machine translations of the input arabic documents can be corrected by identifying and generating from such redundancy , focusing on noun phrases .
	oriented object proposals ; orientations of the object ; detection error ; 	<task> <otherscientificterm> <metric> <otherscientificterm>	3 1 3 ; 3 5 3 ; 3 4 3 ; 3 3 3 ; 3 6 3 ; 2 4 0 ; 3 0 3	in this paper , we propose a new approach to generate <task_0> to reduce the <metric_2> caused by various <otherscientificterm_1> . to this end , we propose to efficiently locate object regions according to pixelwise object probability , rather than measuring the objectness from a set of sampled windows . we formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes -lrb- i.e. , sizes and orientations -rrb- can be produced by locating the local maximum likelihoods . the new approach has three main advantages . first , it helps the object detector handle objects of different orientations . second , as the shapes of the proposals may vary to fit the objects , the resulting proposals are tighter than the sampling windows with fixed sizes . third , it avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high recall . experiments on the pascal voc 2007 dataset show that the proposed oop outperforms the state-of-the-art fast methods . further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios . generating oops is very fast and takes only 0.5 s per image .
	paramax spoken language understanding system ; implicit reference resolution ; database query paraphrase ; non-monotonic reasoning ; 	<method> <task> <task> <task> <material>	4 4 4	this paper describes three relatively domain-independent capabilities recently added to the <method_0> : <task_3> , <task_1> , and <task_2> . in addition , we discuss the results of the february 1992 atis benchmark tests . we describe a variation on the standard evaluation metric which provides a more tightly controlled measure of progress . finally , we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving ocr accuracy .
	fine-grained sketch-based image retrieval ; scarce , challenging many state-of-the-art ; instance-level retrieval of images ; free-hand human sketches ; harder , ; 	<task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task>	5 1 5 ; 3 1 2 ; 5 6 5	we investigate the problem of <task_0> , where <otherscientificterm_3> are used as queries to perform <task_2> . this is an extremely challenging task because -lrb- i -rrb- visual comparisons not only need to be fine-grained but also executed cross-domain , -lrb- ii -rrb- free-hand -lrb- finger -rrb- sketches are highly abstract , making fine-grained matching <otherscientificterm_4> and most importantly -lrb- iii -rrb- annotated cross-domain sketch-photo datasets required for training are <otherscientificterm_1> machine learning techniques . in this paper , for the first time , we address all these challenges , providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application . we introduce a new database of 1,432 sketch-photo pairs from two categories with 32,000 fine-grained triplet ranking annotations . we then develop a deep triplet-ranking model for instance-level sbir with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data . extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks .
	which has a good potential to ; generic action proposals ; a spatio-temporal ; unconstrained videos ;  ; .	<otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>	4 5 4 ; 4 4 4 ; 5 5 0 ; 3 1 1 ; 5 1 4 ; 4 6 4 ; 4 1 4 ; 4 3 4	in this paper we target at generating <otherscientificterm_1> in <material_3> . each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , <otherscientificterm_2> video tube , <otherscientificterm_0> locate one human action <otherscientificterm_5> assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the ac-tionness of the video tubes <otherscientificterm_5> after picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score <otherscientificterm_5> compared with existing action proposal approaches , our action proposals do not rely on video segmentation and can be generated in nearly real-time <otherscientificterm_5> experimental results on two challenging datasets , msrii and ucf 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search .
	competing combinations , and to creation ; creating natural language text ; 	<method> <task> <method>	2 1 2	this paper reports recent research into methods for <task_1> . a new processing paradigm called fragment-and-compose has been created and an experimental system implemented in it . the knowledge to be expressed in text is first divided into small propositional units , which are then composed into appropriate combinations and converted into text . kds -lrb- knowledge delivery system -rrb- , which embodies this paradigm , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among <method_0> of the final text . the fragment-and-compose paradigm and the computational methods of kds are described .
	phrase-based translation models ; phrase-based translation model ; decoding algorithm	<method> <method> <method>	1 6 2	we propose a new <method_1> and <method_2> that enables us to evaluate and compare several , previously proposed <method_0> . within our framework , we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models . our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations . surprisingly , learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance . learning only syntactically motivated phrases degrades the performance of our systems .
	model of reference , ; dialogue system ;  ; collaborative ; . ; semantics	<method> <method> <otherscientificterm> <task> <method> <task>	0 1 5 ; 5 6 3 ; 0 1 3	we describe a <method_1> that works with its interlocutor to identify objects <method_4> our contributions include a concise , modular architecture with reversible processes of understanding and generation , an information-state <method_0> and flexible links between <task_5> and <task_3> problem solving .
